## 2021年10月16日总结

### 任务

1. 整理总结GGNN与RGCN的公式，尝试反推出代码中的参数含义。
2. 继续运行调试理解剩下的五个model。

### 公式解读

1. paper中给出的例子，来对公式进行解读：

   - graph在各种GNN及变形后，node A的representation A，变为了A‘：

     <img src="D:\APP\Typora\images_data\1634711676826.png" alt="1634711676826" style="zoom:67%;" />

   - A所关联的node是B、C、D，公式是这样计算的：

     <img src="D:\APP\Typora\images_data\1634711708764.png" alt="1634711708764" style="zoom:80%;" />

2. 笔记图片：

   <img src="D:\APP\Typora\images_data\1634732025088.png" alt="1634732025088" style="zoom: 33%;" />

   <img src="D:\APP\Typora\images_data\1634732149113.png" alt="1634732149113" style="zoom:33%;" />

   <img src="D:\APP\Typora\images_data\1634732204709.png" alt="1634732204709" style="zoom:33%;" />

   <img src="D:\APP\Typora\images_data\1634732249608.png" alt="1634732249608" style="zoom:33%;" />

### 参数解读

1. 下面的graph_residual_connection_every_num_layers，是建立残差连接（residual connection），目的是防止梯度消失。模的原因是每间隔多少层进行一次残差连接。

   ```python
   if layer_idx % self.params['graph_residual_connection_every_num_layers'] == 0:  #？这一步又是什么目的？ #2 or 10000
   	t = cur_node_representations  ####可能是 图或者矩阵 的专有名词   为什么？
       if layer_idx > 0:
       	cur_node_representations += last_residual_representations
           cur_node_representations /= 2
       last_residual_representations = t#这上面可以理解为对cur_node_representations做了一定的变换操作
   ```

   - 关于梯度消失（ gradient vanishing）：与梯度爆炸类似，都是出现在层数比较多的neural network，距离output layer较近的hidden layer5的权值更新比较正常，但是距离output layer比较远的hidden layer1的权值更新会很慢，几乎不变，以至于hidden layer1只是相当于一个映射层。

     - 理论方面：

       对于：

        <img src="https://pic3.zhimg.com/v2-b9e0d6871fbcae05d602bab65620a3ca_r.jpg" alt="preview" style="zoom: 67%;" /> 

       可以推导出：

       <img src="D:\APP\Typora\images_data\1634439260658.png" alt="1634439260658" style="zoom:50%;" />

       当activation function为sigmoid时：

       <img src="D:\APP\Typora\images_data\1634439315468.png" alt="1634439315468" style="zoom:67%;" />

   - 关于residual connection： 假设神经网络某一层对input x进行了一个F操作，变为F(x)，那么正常的神经网络输出为F(x)，而加入残差连接以后，输出为x+F(x)；显而易见：因为增加了一项，那么该层网络对x求偏导的时候，多了一个常数项，所以在反向传播过程中，梯度连乘，也不会造成梯度消失。 

2. 下面的graph_dense_between_every_num_gnn_layers，是为了配置 how often a per-node representation dense layer is inserted between the message passing layers。

   ```python
   #设置节点表示层在message pasing layer中插入的频率
   if layer_idx % self.params['graph_dense_between_every_num_gnn_layers'] == 0:     
   	cur_node_representations = \ 
   		tf.keras.layers.Dense(units=h_dim,
   							  use_bias=False,                              										  activation=activation_fn,                              							   name="Dense",)(cur_node_representations)
   ```

### model理解

1. 每个model中，先让不同type的edge经过一个不同的Dense（或者是MLP）的原因是为了区分下面的情况：
   
   ```python
   G_1 = (V={1, 2, 3}, E_1={(1, 2)}, E_2={(3, 2)}) and 
   G_2 = (V={1, 2, 3}, E_1={(3, 2)}, E_2={(1, 2)}). 
   If we would treat all edges the same,G_1.E_1 \cup G_1.E_2 == G_2.E_1 \cup G_2.E_2 would imply that the two graphsbecome indistuingishable.
   ```
   
2. RGAT（relational graph attention ：

   - attention机制的好处：可以通过找出最相关部分，来处理可变长度的输入。

3. MLP（多层感知机）就是最简单的neural network，包括input layer, hidden layer, output layer。

4. Hypernetworks（超网络）：一种使用一个网络（也称为超网络）为另一个网络生成权重的方法。

### 其它

1. 找到了tf-gnn-samples的tensorflow2实现，其中交代了一些参数的含义：https://github.com/microsoft/tf2-gnn

2. layer normalization：针对深度网络的某一层的所有neuron的输入按以下公式进行normalization操作：

    <img src="https://img-blog.csdn.net/20180714180615653?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpdXhpYW8yMTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" style="zoom: 50%;" /> 

3. 关于tf.variable_scope()和tf.get_variable()：

    - get_variable()：建立或者获取一个共享变量，如果建立的变量名已经存在，且该变量不是共享变量，就会报错。
    - variable_scope()：通过变量作用域（variable scope）实现对变量的共享和管理。 例如，cnn的每一层中，均有weights和biases这两个变量，通过tf.variable_scope()为每一卷积层命名，就可以防止变量命名重复。 

4. tf.unsorted_segment_sum(data, segment_ids, num_segments, name=None)：
   - data是要处理的tensor；segment_ids也是一个tensor，对应元素的值标志着data的下标；num_segments是一个tensor，对应着最后生成结果的维度。
   - 计算一个张量,使得 output[i] = data[index] 。其中index为对应segment_ids中元素值为i的位置。如果值i在segment_ids中不存在，则output[i] =0 ； num_segments 应等于不同的段 ID 的数量 

5. theorem：定理，就是正确的话；

   lemma：引理，比较小的定理，往往是从定理中拆分出来的。

6. python的换行符 \ 后是不能够有任何字符的，包括空格，否则会报错。

### 问题

1. ```python
   tf.unsorted_segment_sum(data=tf.expand_dims(attention_values, -1) * messages,
   ```

   这里为什么要增加一个维度？

   解答：如果不添加一个维度的话，就是[M] * [M, D/K]，这样会报错，但是扩展一个维度后就变成了[M,1] * [M, D/K]，这样得到的结果是[M, D/K]，我猜是自动对[M, 1]进行了扩展。
   
2. 这里和paper中有点不同

   ![1634546621552](D:\APP\Typora\images_data\1634546621552.png)

<img src="D:\APP\Typora\images_data\1634546702445.png" alt="1634546702445" style="zoom: 67%;" />

3. 