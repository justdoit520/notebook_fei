## 2021.10.7笔记

### 任务：

- 每个程序一个独立的环境，避免版本问题

- 全英文路径

- ad反编译的目标文件是可以选择的，根据需要来选择

- 先跑起来tf-gnn，来进一步了解tf、keras，感受一下

  跑程序的时候了解思路

  目标是工程

- 再去看codebert

- 第二个往后放，重点在code decoder

- anaconda的使用

### 关于beam search：

- 链接：https://zhuanlan.zhihu.com/p/82829880

- 一种搜索方法，拥有一个超参数beam size，比如为k，就取概率最大的k个候选，用于下一步的搜索，直到进行到最后。

### 关于Anaconda：

- 链接： [Anaconda详细安装及使用教程（带图文）_代码帮-CSDN博客](https://blog.csdn.net/ITLearnHall/article/details/81708148) 

- Anaconda指的是一个开源的Python发行版本，其包含了conda、Python等180多个科学包及其依赖项 
- conda是一个环境管理的系统，可以类比于pip，可以为conda定义不同的源。

### 关于配置环境：

每个project都要有自己独立的配置环境，先用conda创建一个新的环境，对于缺少的库，现尝试用conda补全，conda源中没有的库，再尝试使用pip。

### GNN

- GNN采用在每个节点上分别传播的方式进行学习，由此忽略了节点的顺序。
- GNN常通过邻居节点的加权求和来更新节点的隐藏状体。

### 深度学习

- 优化器的作用：调节w和b，使得loss尽可能地小就是optimizer的作用，常见的有Adam、Sgd、Rmsprop等。

- tf.placeholder()：每一个tensor值在graph上都是一个op，

- tensorflow程序通常被分为构建阶段和执行阶段；在构建阶段，op的执行步骤被描述成一个图，在执行阶段，使用会话（session）执行图中的op。feeds和fetches可以为任意的op赋值或者从其中获取数据。

- graph定义了计算方式，是一些加减乘除等运算的组合，类似于一个函数。它本身不会进行任何计算，也不保存任何中间计算结果。session用来运行一个graph，或者运行graph的一部分。它类似于一个执行者，给graph灌入输入数据，得到输出，并保存中间的计算结果。同时它也给graph分配计算资源（如内存、显卡等）。

- tensorflow提供了Variable Scope这种独特的机制来共享变量，主要涉及两个函数：

  ```tf
  tf.get_variable(<name>, <shape>, <initializer>) 创建或返回给定名称的变量
  tf.variable_scope(<scope_name>) 管理传给get_variable()的变量名称的作用域
  ```

### tf-gnn-samples

- 用不同的模型完成了四项任务：

  - citation networks（引用网络）：

    The implementation illustrates how to handle the case of transductive graph learning on a single graph instance by masking out nodes that shouldn't be considered. 

  - PPI：

    The implementation illustrates how to handle the case of inductive graph learning with node-level predictions. 

  - QM9：

    The implementation illustrates how to handle the case of inductive graph learning with graph-level predictions. 

    跑出理想的结果可能需要几天。

  - VarMisuse：

    The implementation illustrates how to handle the case of inductive graph learning with predictions based on node selection.

    并没有完全复现，也需要相对比较长的时间。

- 跑通情况：

  |              | citation networks | PPI  | QM9                      | VarMisuse |
  | ------------ | ----------------- | ---- | ------------------------ | --------- |
  | GGNN         | √                 | √    | √                        | 13G .zip  |
  | RGCN         |                   |      | √                        |           |
  | RGAT         |                   |      | √                        |           |
  | RGIN         |                   |      | √                        |           |
  | GNN-Edge-MLP |                   |      | √                        |           |
  | RGDCN        |                   |      | batch_size太大，内存不够 |           |
  | GNN-FiLM     |                   |      | √                        |           |

  

- RGCN：适用于有大规模关系数据（relation的类型也有不同）的graph。

  <img src="D:\APP\Typora\images_data\1634267140770.png" alt="1634267140770" style="zoom: 67%;" />

  这个公式，对于一个node i，和以不同的关系r关联起来的node j，分关系类型地加在一起，再加上本身的信息h<sub>i</sub><sup>(l)</sup>，就构成了新的node feature：h<sub>i</sub><sup>(l+1)</sup>。

  <img src="D:\APP\Typora\images_data\1634267607372.png" alt="1634267607372" style="zoom:50%;" />

- 如果跑通的话数据结果应该都是可以复现的，自己跑了两次，从log文件看数据完全相同。

- 程序内容：

  - MODEL_NAME关键字：确定使用的模型，也就是下图之一。

    <img src="D:\APP\Typora\images_data\1633763223053.png" alt="1633763223053" style="zoom:50%;" />

  - TASK_NAME关键字：确定要完成的任务。


### 名词解释：

- transductive learning：与inductive learning（归纳式学习）相对应，在训练时已经用到了测试集数据（unlabelled）的信息，这样可以从特征分布上学到额外的信息，但是只要有新的样本来，模型就得重新训练。

- python相关：

  - 函数后面的“ -> ”，是为函数添加元数据，描述函数的返回类型；可以在参数后加上 ：和数据类型来指定参数的数据类型，加等号表示初始值，比如：

    ```python
    def fun(num_timesteps: int = 1)->int
    ```
    
  - raise关键字：用来抛出异常，后续的代码将无法执行。
  
  - with关键字：是一种上下文管理协议，目的在于去掉try、except、finally等关键字去掉，进化流程。
  
  - numpy中，[:, a]，可以理解为在矩阵中取第a列的元素。
  
- MAE：平均绝对误差（mean absolute error），观测值与真实值的误差绝对值的平均值。

  <img src="D:\APP\Typora\images_data\1633833653810.png" alt="1633833653810" style="zoom:50%;" />



## 问题

1. 在执行 python train.py GGNN QM9 的时候为什么会出现train loss高于valid loss，是否是正常的？

   （这里是loss，从最开始的随机参数训练，每个batch的loss自然是比较大的，而valid loss，是用训练后的模型验证得到的loss，自然就会小一些）

   <img src="D:\APP\Typora\images_data\1633763768273.png" alt="1633763768273" style="zoom:80%;" />

2. 看了一些资料，还是不能理解with variable_scope() 的用法：

   <img src="D:\APP\Typora\images_data\1634108122698.png" alt="1634108122698" style="zoom:80%;" />

3. sparse_graph_model()中对于一些参数的运用没有理解？比如graph_residual_connection_every_num_layers

   174行：

   ![1634209776093](D:\APP\Typora\images_data\1634209776093.png)

   194行：参数用处；待确定：这里设置的Dense就是单纯地乘了一个W矩阵，这里再使用一个activation。

   ![1634209527836](D:\APP\Typora\images_data\1634209527836.png)